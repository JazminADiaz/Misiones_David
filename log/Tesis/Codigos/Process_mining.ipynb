{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All information needed will be added in this code, each code has a markdown on top of it for an explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some libraries are added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import os, glob\n",
    "\n",
    "import shutil\n",
    "from IPython.utils.io import capture_output\n",
    "import random\n",
    "\n",
    "#from my_simulation import simulacion  \n",
    "\n",
    "#This is the only line for the directory you should need to change\n",
    "dir_tutti=\"/home/jazmin/tuttifrutti\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we add the activities of the mision, and the probability of success\n",
    "\n",
    "Go ahead and select option for a random collection of activities or use the example given (modify if like, the code will still work)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the word \"manual\" or \"random\"\n",
    "mision_activities=\"random\"\n",
    "\n",
    "#probability from 0 to 1\n",
    "fixed_prob=\"probability=0.8;\"\n",
    "\n",
    "#number of times the mision is repeated to be stored in one log\n",
    "repetition_numer=10\n",
    "\n",
    "#duration of the simulation in seconds\n",
    "timer=20000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the activities, this function will be called, if you selected a \"manual\" choice of the activities, here you can modify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "def activities():\n",
    "    if mision_activities==\"manual\":   \n",
    "        ################################ ADD THE ACTIVITIES #######################################################     #delete previos content\n",
    "        file=dir_tutti+\"/log/Tesis/mision_variables/activities.txt\"\n",
    "        with open(file, \"w\") as f:\n",
    "            f.write(\"activities[\\\"0_sec\\\"] = {0,22,23};\")\n",
    "            f.write(\"\\n\") \n",
    "            f.write(\"activities[\\\"1_con\\\"] = {31,25,26};\")\n",
    "\n",
    "            ################################ ADD THE ACTIVITIES #######################################################\n",
    "\n",
    "    if mision_activities==\"random\":\n",
    "        numbers_total = list(range(2, 30 + 1))\n",
    "        numbers_list=list(range(2, 30 + 1))\n",
    "        weights_t = [1] * len(numbers_total)\n",
    "        weights_l = [1] * len(numbers_list)\n",
    "\n",
    "        max_total_values = random.choices(numbers_total, weights_t)[0]\n",
    "        print(f\"max: {max_total_values}\" )\n",
    "\n",
    "\n",
    "        sum_total = 0\n",
    "        dictionary_lists = {}\n",
    "\n",
    "        list_counter = 0\n",
    "        initial_word = random.choice([\"con\\\"\", \"sec\\\"\"])\n",
    "        previous_word = initial_word\n",
    "\n",
    "        while sum_total != max_total_values:\n",
    "            # Genera una lista de números sin repetición\n",
    "            minimo=min(30, max_total_values - sum_total)\n",
    "            if (minimo==1):\n",
    "                break\n",
    "            numbers_random = list(range(2, min( 30, max_total_values - sum_total) + 1))\n",
    "            # Asigna el mismo peso a cada número (en este caso, 1)\n",
    "            weights_n = [1] * len(numbers_random)\n",
    "\n",
    "            # Elige un número aleatorio con igual probabilidad\n",
    "            random_number = random.choices(numbers_random, weights_n)[0]\n",
    "\n",
    "            if sum_total + random_number <= max_total_values:\n",
    "                sum_total += random_number\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if previous_word == \"con\\\"\":\n",
    "                current_word = \"sec\\\"\"\n",
    "            else:\n",
    "                current_word = \"con\\\"\"\n",
    "\n",
    "            key = f\"\\\"{list_counter}_{current_word}\"\n",
    "\n",
    "            # Genera números aleatorios con igual probabilidad utilizando random.choices\n",
    "            if current_word == \"con\\\"\":\n",
    "                # Crear población excluyendo valores ya seleccionados en \"con\\\"\"\n",
    "                population = set(range(60)) - set(dictionary_lists.get(f\"\\\"{list_counter}_con\\\"\", []))\n",
    "                random_numbers = random.choices(list(population), k=random_number)\n",
    "            else:\n",
    "                random_numbers = random.choices(range(60), k=random_number)\n",
    "\n",
    "            dictionary_lists[key] = random_numbers\n",
    "            list_counter += 1\n",
    "            previous_word = current_word\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Total sum:\", sum_total)\n",
    "        print(\"Dictionary of lists:\")\n",
    "\n",
    "        file=dir_tutti+\"/log/Tesis/mision_variables/activities.txt\"\n",
    "        with open(file, \"a\") as f:        \n",
    "            for key, lista in dictionary_lists.items():\n",
    "                print(f'{key}: {lista}')  \n",
    "                lista_str = ', '.join(map(str, lista))\n",
    "                f.write(\"\\n\") \n",
    "                f.write(\"activities[\" + key + \"] = {\" + lista_str + \"};\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "execute the mision, you can change variable n_repetition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next line is executed when option manual is used, change the folder path to your needs\n",
    "\n",
    "\n",
    "you can change the number of repetitions of the mision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=dir_tutti+\"/log/Tesis/mision_variables/activities.txt\"\n",
    "file_send=dir_tutti+\"/log/Tesis/EventLogs/\"\n",
    "with open(file, \"w\") as f:\n",
    "    f.write(\"probability=1;\")\n",
    "activities()\n",
    "!python /home/jazmin/tuttifrutti/log/Tesis/Codigos/seed_change_multiple_times.py --directorio $file_send --repetition $repetition_numer  --time $timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=dir_tutti+\"/log/Tesis/mision_variables/activities.txt\"\n",
    "file_send=dir_tutti+\"/log/Tesis/EventLogs_fail/\"\n",
    "with open(file, \"r\") as archivo:\n",
    "    lineas = archivo.readlines()\n",
    "lineas[0] = fixed_prob+\"\\n\"\n",
    "with open(file, \"w\") as archivo:\n",
    "    archivo.writelines(lineas)\n",
    "!python /home/jazmin/tuttifrutti/log/Tesis/Codigos/seed_change_multiple_times.py --directorio $file_send --repetition $repetition_numer  --time $timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store all information for the mision in one folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create new folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where you want to search for folders\n",
    "directory = dir_tutti+\"/log/Tesis/\"\n",
    "\n",
    "# Prefix that should be in the folder names\n",
    "prefix = \"example\"\n",
    "\n",
    "# Find all folders with the \"example\" prefix in the name\n",
    "folders = [name for name in os.listdir(directory) if prefix in name]\n",
    "\n",
    "# Find the highest number in the folder names\n",
    "numbers = [int(name.replace(prefix, \"\")) for name in folders]\n",
    "highest_number = max(numbers) if numbers else 0\n",
    "\n",
    "# Create the name for the new folder\n",
    "new_name = f\"{prefix}{highest_number + 1}\"\n",
    "\n",
    "# Full path of the new folder\n",
    "new_folder = os.path.join(directory, new_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to copy folder into the new folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_folder(source, destination):\n",
    "    try:\n",
    "        # Create the new folder if it doesn't exist\n",
    "        if not os.path.exists(destination):\n",
    "            os.mkdir(destination)\n",
    "        # Use the `copytree` function from shutil to copy the folder and its contents\n",
    "        shutil.copytree(source, os.path.join(destination, os.path.basename(source)))\n",
    "        print(f\"Folder '{os.path.basename(source)}' copied to {destination}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to copy the folder: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to copy a file into a new folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_file(source, destination):\n",
    "    try:\n",
    "        # Use the `copy` function from shutil to copy the file\n",
    "        shutil.copy(source, destination)\n",
    "        print(f\"File copied from {source} to {destination}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to copy the file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files to copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_log = dir_tutti+\"/log/Tesis/EventLogs\"\n",
    "event_log_fail =dir_tutti+\"/log/Tesis/EventLogs_fail\"\n",
    "variables=dir_tutti+\"/log/Tesis/mision_variables/activities.txt\"\n",
    "\n",
    "copy_folder(event_log, new_folder)\n",
    "copy_folder(event_log_fail, new_folder)\n",
    "copy_file(variables, new_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next line is executed when option random is used, change the folder path to your needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rest of the time, executed from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import seaborn as sns\n",
    "from datetime import date\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.visualization.petri_net import visualizer as pn_visualizer\n",
    "from pm4py.objects.petri_net.utils.decomposition import decompose\n",
    "from pm4py.objects.petri_net.utils import reachability_graph\n",
    "from pm4py.visualization.transition_system import visualizer as ts_visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one dataframe with the info from all EventLogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def concatenate(folder):\n",
    "    filenames = glob.glob(folder + \"/*.csv\")\n",
    "    dfs = []\n",
    "    model=0\n",
    "    for filename in filenames:\n",
    "        log = pd.read_csv(filename, sep=',')\n",
    "        case_id=[]\n",
    "        case_id += len(log.mision) * [\"mision \"+str(model)]\n",
    "        log['mision']=case_id\n",
    "        model+=1\n",
    "        dfs.append(log)\n",
    "        big_frame = pd.concat(dfs, ignore_index=True)\n",
    "        if model==len(filenames):\n",
    "            big_frame.to_csv(dir_tutti+'/log/Tesis/LogFinal/final_log_2.csv', sep=',', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the names for the colums in both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = dir_tutti+'/log/Tesis/LogFinal/final_log_2.csv'\n",
    "concatenate(new_folder+\"/EventLogs\")\n",
    "events = pd.read_csv(fn)\n",
    "events.columns = ['mision', 'action', 'datetime', 'resource', 'random_seed']\n",
    "events['datetime'] = pd.to_datetime(events['datetime'])\n",
    "events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concatenate(new_folder+\"/EventLogs_fail\")\n",
    "events_fail = pd.read_csv(fn)\n",
    "events_fail.columns = ['mision', 'action', 'datetime', 'resource', 'random_seed']\n",
    "events_fail['datetime'] = pd.to_datetime(events_fail['datetime'])\n",
    "events_fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use pm4py first we convert to an EventLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.rename(columns={'datetime': 'time:timestamp', 'mision': 'case:concept:name', 'action': 'concept:name', 'random_seed': 'org:resource'}, inplace=True)\n",
    "events_fail.rename(columns={'datetime': 'time:timestamp', 'mision': 'case:concept:name', 'action': 'concept:name', 'random_seed': 'org:resource'}, inplace=True)\n",
    "\n",
    "## Convert to log format the events without fail probability\n",
    "log_inicial = log_converter.apply(events)\n",
    "## Convert to log format the events with fail probability\n",
    "log_fail = log_converter.apply(events_fail)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ideal log, we don't need much information, just to make sure is perfect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check for the variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = pm4py.get_variants(log_fail)\n",
    "len(variants)\n",
    "variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be as many as number of (concurrent activities x number of actions on the activities) the mision has, if there's more, substrac them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "log = pm4py.filter_variants_top_k(log_inicial, k)\n",
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_no_timeout = pm4py.filter_activities_rework(log_inicial, \"time_out\", 1)\n",
    "log_no_timeout = pm4py.filter_event_attribute_values(log_inicial, \"concept:name\", [\"time_out\"], level=\"case\", retain=False)\n",
    "log_no_timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the model using different algorithims, we'll use 4: alpha miner, inductive miner, heuristic miner, lip miner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuristic miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Petri net\n",
    "net, im, fm = pm4py.discover_petri_net_heuristics(log_no_timeout, dependency_threshold=0.99)\n",
    "parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: \"png\"}\n",
    "gviz = pn_visualizer.apply(net, im, fm , parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=log_no_timeout)\n",
    "pn_visualizer.save(gviz, new_folder+\"/Heuristic_miner_frequency_IDEAL.png\")\n",
    "#pm4py.view_petri_net(net, im, fm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ILP miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, im, fm = pm4py.discovery.discover_petri_net_ilp(log_inicial)\n",
    "pm4py.view_petri_net(net, im, fm)\n",
    "#parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: \"png\"}\n",
    "#gviz = pn_visualizer.apply(net, im, fm, parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=log_inicial)\n",
    "#pn_visualizer.save(gviz, new_folder+\"/ILP_miner_frequency_IDEAL.png\")\n",
    "#pm4py.view_petri_net(net, im, fm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inductivo miner\n",
    "\n",
    "We'll use inductive miner at the end beause we'll use it for conformance checking and like this variables as net, initial_marking, final_marking are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, initial_marking, final_marking = pm4py.discover_petri_net_inductive(log_inicial)\n",
    "pm4py.view_petri_net(net, initial_marking, final_marking)\n",
    "\n",
    "#we can add frequency and performance, change the word to FREQUENCY or PERFORMANCE accordingly\n",
    "#parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: \"png\"}\n",
    "#gviz = pn_visualizer.apply(net, initial_marking, final_marking, parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=log_inicial)\n",
    "#pn_visualizer.save(gviz, new_folder+\"/Inductive_miner_frequency_IDEAL.png\")\n",
    "#pn_visualizer.view(gviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, initial_marking, final_marking = pm4py.discover_petri_net_inductive(log_no_timeout)\n",
    "pm4py.view_petri_net(net, initial_marking, final_marking)\n",
    "\n",
    "#we can add frequency and performance, change the word to FREQUENCY or PERFORMANCE accordingly\n",
    "#parameters = {pn_visualizer.Variants.PERFORMANCE.value.Parameters.FORMAT: \"png\"}\n",
    "#gviz = pn_visualizer.apply(net, initial_marking, final_marking, parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=log_no_timeout)\n",
    "#pn_visualizer.save(gviz, new_folder+\"/Inductive_miner_frequency_IDEAL_not_time_out.png\")\n",
    "#pn_visualizer.view(gviz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an IDEAL model of the operation, we'll get some variables for the real proccess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REAL LOG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To only get activities that happen fast from 0 to 200 seconds only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_log = pm4py.filter_case_performance(log_fail, 0, 100)\n",
    "filtered_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of varians, the activities involved (with its frequency) and the resource (with its frequency)\n",
    "\n",
    "\n",
    "Note: the resource can be exchange between robot and random_seed, to do so go back to the log converter and change the name of the resource\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = pm4py.get_variants(log_fail)\n",
    "activities = pm4py.get_event_attribute_values(log_fail, \"concept:name\")\n",
    "resources = pm4py.get_event_attribute_values(log_fail, \"org:resource\")\n",
    "len(variants)\n",
    "activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the probability of failure is too high you may enconter filtering the log to only the most k used variables useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=len(variants)\n",
    "log_variantes = pm4py.filter_variants_top_k(log_fail, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets say you you want to filter your log only on a especific attribute.\n",
    "You can do it by case meaning all the cases that have that attribute, or by event by filtering only the events containing the attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can get all the events containing a attribute\n",
    "tracefilter_log_event = pm4py.filter_event_attribute_values(log_fail, \"org:resource\", [7], level=\"event\", retain=False)\n",
    "#Or all the cases containing a attribute\n",
    "tracefilter_log_case = pm4py.filter_event_attribute_values(log_fail, \"org:resource\", [7], level=\"case\", retain=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to check for cases were two activities are follow by one anohter we use this filter, if you can check for example how many times an activity rebooted when x activity happend first, yo can do it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Log_between = pm4py.filter_between(log_fail, \"T_0_Busy\", \"T_0_rebooting\")\n",
    "len(filtered_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To eliminate noise, we can eliminte logs that are too long (rework) or too short (incompleate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_by_size = pm4py.filter_case_size(log_fail, 0, 9)\n",
    "filtered_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check for activities being excecuted more than k time we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "filtered_log = pm4py.filter_activities_rework(log_fail, 'T_0_rebooting', k)\n",
    "print(filtered_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check for paths that take too long we use paths performance, so we get the list of paths taking longer than expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_log = pm4py.filter_paths_performance(log_fail, (\"T_0_Done\", \"T_2_Busy\"), 0, 200)\n",
    "filtered_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also model the REAL log, we'll use the same 4 algorithms, but we'll only save the inductive, since it works better and this models won't be used in conformance checking tecniches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INDUCTIVE MINER\n",
    "\n",
    "This algorithim can be seen as a petri net, as a process tree and as a bpmn model.\n",
    "According to the state of the art literature this alg should be the best at handeling the behaviour of a group of robots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_f, initial_marking_f, final_marking_f = pm4py.discover_petri_net_inductive(log_fail)\n",
    "\n",
    "#we can add frequency and performance, change the word to FREQUENCY or PERFORMANCE accordingly\n",
    "parameters = {pn_visualizer.Variants.PERFORMANCE.value.Parameters.FORMAT: \"png\"}\n",
    "gviz = pn_visualizer.apply(net_f, initial_marking_f, final_marking_f, variant=pn_visualizer.Variants.FREQUENCY, log=log_fail)\n",
    "pn_visualizer.view(gviz)\n",
    "pn_visualizer.save(gviz, new_folder+\"/Inductive_miner_frequency_REAL.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = pm4py.discover_process_tree_inductive(log_fail)\n",
    "pm4py.view_process_tree(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpmn_graph = pm4py.discover_bpmn_inductive(log_fail)\n",
    "#pm4py.view_bpmn(bpmn_graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    HEURISTIC MINER\n",
    "\n",
    "    Heuristics Miner is an algorithm that acts on the Directly-Follows Graph, providing way to handle with noise and to find common constructs (dependency between two activities, AND).\n",
    "\n",
    "    We will obtain a Heuristic net and a Petri Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heuristic net\n",
    "heu_net = pm4py.discover_heuristics_net(log_fail, dependency_threshold=0.99)\n",
    "#pm4py.view_heuristics_net(heu_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Petri net\n",
    "net_f, initial_marking_f, final_marking_f= pm4py.discover_petri_net_heuristics(log_fail, dependency_threshold=0.99)\n",
    "#pm4py.view_petri_net(net_f, initial_marking_f, final_marking_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ILP MINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_f, initial_marking_f, final_marking_f = pm4py.discovery.discover_petri_net_ilp(log_fail)\n",
    "#pm4py.view_petri_net(net_f, initial_marking_f, final_marking_f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A great tool that allows us to check exactly activities that follow one another are directly follows diagrams, we have some bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg, start_activities, end_activities = pm4py.discover_dfg(log_fail)\n",
    "#pm4py.view_dfg(dfg, start_activities, end_activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to add performance (average time) to the dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_dfg, start_activities, end_activities = pm4py.discover_performance_dfg(log_fail)\n",
    "#pm4py.view_performance_dfg(performance_dfg, start_activities, end_activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model is too complicated, but you want to analize it visually you can decompose it into smaller pieces, here you have the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net_f, initial_marking_f, final_marking_f = pm4py.discover_petri_net_inductive(log_fail)\n",
    "list_nets = decompose(net_f, initial_marking_f, final_marking_f)\n",
    "for index, model in enumerate(list_nets):\n",
    "    subnet, s_im, s_fm = model\n",
    "    #pm4py.view_petri_net(subnet, s_im, s_fm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "All that you just did is contamplated in process mining as process discovery, hope it was fun, there are many many other tools you can use, check the pm4py website if you want to explore them further.\n",
    "\n",
    "Now we will be doing Conformance Checking, here we will use our REAL LOG and our IDEAL MODEL, we will compare them and if there are anomalies as we expect, we'll try to detect them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conformance Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token-based replay\n",
    "\n",
    " Based on this comparison, token replay provides insights into whether the recorded process adheres to the expected process model. It can identify deviations, missing steps, or extra steps in the actual process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Create a list to store trace results\n",
    "trace_results_list = []\n",
    "\n",
    "replayed_traces = pm4py.conformance_diagnostics_token_based_replay(log_fail, net, initial_marking, final_marking)\n",
    "\n",
    "# Iterate through the replayed traces and access conformance metrics for each trace\n",
    "for i, trace_result in enumerate(replayed_traces, start=1):\n",
    "    trace_is_fit = trace_result['trace_is_fit']  # Boolean indicating if the trace fits the model\n",
    "    trace_fitness = trace_result['trace_fitness']  # Fitness value for the trace\n",
    "    missing_tokens = trace_result['missing_tokens']  # Number of missing tokens\n",
    "    consumed_tokens = trace_result['consumed_tokens']  # Number of consumed tokens\n",
    "    remaining_tokens = trace_result['remaining_tokens']  # Number of remaining tokens\n",
    "    produced_tokens = trace_result['produced_tokens']  # Number of produced tokens\n",
    "\n",
    "    # Create a dictionary to store trace results\n",
    "    trace_result_dict = {\n",
    "        \"Trace Number\": i,\n",
    "        \"Trace Is Fit\": trace_is_fit,\n",
    "        \"Trace Fitness\": trace_fitness,\n",
    "        \"Missing Tokens\": missing_tokens,\n",
    "        \"Consumed Tokens\": consumed_tokens,\n",
    "        \"Remaining Tokens\": remaining_tokens,\n",
    "        \"Produced Tokens\": produced_tokens\n",
    "    }\n",
    "\n",
    "    # Append the trace result dictionary to the list\n",
    "    trace_results_list.append(trace_result_dict)\n",
    "\n",
    "    # Access other information as needed\n",
    "    activated_transitions = trace_result['activated_transitions']\n",
    "    reached_marking = trace_result['reached_marking']\n",
    "    transitions_with_problems = trace_result['transitions_with_problems']\n",
    "    enabled_transitions_in_marking = trace_result['enabled_transitions_in_marking']\n",
    "\n",
    "# Define the path to the CSV file\n",
    "csv_file_path = new_folder+\"/trace_results.csv\"\n",
    "\n",
    "# Define the CSV fieldnames\n",
    "fieldnames = [\"Trace Number\", \"Trace Is Fit\", \"Trace Fitness\", \"Missing Tokens\", \"Consumed Tokens\", \"Remaining Tokens\", \"Produced Tokens\"]\n",
    "\n",
    "# Write the trace results to a CSV file with separators\n",
    "with open(csv_file_path, mode='w', newline='') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    \n",
    "    # Write the header row\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for trace_result_dict in trace_results_list:\n",
    "        # Write the trace results\n",
    "        writer.writerow(trace_result_dict)\n",
    "        \n",
    "        # Write a separator row\n",
    "        writer.writerow({key: '-' * 10 for key in fieldnames})\n",
    "\n",
    "print(f\"Trace results have been exported to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagnostics (TBR)\n",
    "\n",
    "\n",
    "\"The execution of token-based replay in pm4py permits to obtain detailed information about transitions that did not execute correctly, or activities that are in the log and not in the model. In particular, executions that do not match the model are expected to take longer throughput time.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.algo.conformance.tokenreplay import algorithm as token_based_replay\n",
    "net, initial_marking, final_marking = pm4py.discover_petri_net_inductive(log_inicial)\n",
    "log_fail = log_converter.apply(events_fail)\n",
    "\n",
    "parameters_tbr = {token_based_replay.Variants.TOKEN_REPLAY.value.Parameters.DISABLE_VARIANTS: True, token_based_replay.Variants.TOKEN_REPLAY.value.Parameters.ENABLE_PLTR_FITNESS: True}\n",
    "replayed_traces, place_fitness, trans_fitness, unwanted_activities = token_based_replay.apply(log_fail, net,\n",
    "                                                                                                initial_marking,\n",
    "                                                                                                final_marking,\n",
    "                                                                                                parameters=parameters_tbr)\n",
    "\n",
    "print(trans_fitness)\n",
    "\n",
    "csv_file_path = new_folder+\"/trans_fitness.csv\"\n",
    "\n",
    "# Open the CSV file for writing and create a CSV writer\n",
    "with open(csv_file_path, mode='w', newline='') as csv_file:\n",
    "    fieldnames = ['Underfed_Traces', 'Fit_Traces']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "    # Write the header row to the CSV file\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Iterate through the trans_fitness dictionary and write each entry to the CSV file\n",
    "    for fitness_data in trans_fitness.values():\n",
    "        writer.writerow({\n",
    "            'Underfed_Traces': str(fitness_data['underfed_traces']),\n",
    "            'Fit_Traces': str(fitness_data['fit_traces'])\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughput analysis (unfit execution)\n",
    "\n",
    "To perform throughput analysis on the transitions that were executed unfit, and then print on the console the result, the following code could be used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughput analysis (activities)\n",
    "\n",
    "\n",
    "To perform throughput analysis on the process executions containing activities that are not in the model, and then print the result on the screen, the following code could be used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alingments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net, initial_marking, final_marking = pm4py.discover_petri_net_inductive(log_no_timeout)\n",
    "#aligned_traces = pm4py.conformance_diagnostics_alignments(log_fail, net, initial_marking, final_marking)\n",
    "#pm4py.fitness_alignments(log_fail, net, initial_marking, final_marking)\n",
    "#aligned_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pm4py import util\n",
    "from pm4py.algo.conformance import alignments as ali\n",
    "from pm4py.algo.conformance.alignments.petri_net.variants.state_equation_a_star import Parameters\n",
    "from pm4py.objects import log as log_lib\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.petri_net.importer import importer as petri_importer\n",
    "from pm4py.objects.petri_net.utils.align_utils import pretty_print_alignments\n",
    "\n",
    "\n",
    "def align(trace, net, im, fm, model_cost_function, sync_cost_function):\n",
    "    trace_costs = list(map(lambda e: 1000, trace))\n",
    "    params = dict()\n",
    "    params[util.constants.PARAMETER_CONSTANT_ACTIVITY_KEY] = log_lib.util.xes.DEFAULT_NAME_KEY\n",
    "    params[Parameters.PARAM_MODEL_COST_FUNCTION] = model_cost_function\n",
    "    params[Parameters.PARAM_TRACE_COST_FUNCTION] = trace_costs\n",
    "    params[Parameters.PARAM_SYNC_COST_FUNCTION] = sync_cost_function\n",
    "    return ali.petri_net.algorithm.apply_trace(trace, net, im, fm, parameters=params,\n",
    "                                   variant=ali.petri_net.algorithm.VERSION_STATE_EQUATION_A_STAR)\n",
    "\n",
    "\n",
    "def execute_script():\n",
    "\n",
    "    model_cost_function = dict()\n",
    "    sync_cost_function = dict()\n",
    "    for t in net.transitions:\n",
    "        if t.label is not None:\n",
    "            model_cost_function[t] = 1000\n",
    "            sync_cost_function[t] = 0\n",
    "        else:\n",
    "            model_cost_function[t] = 1\n",
    "\n",
    "    alignments = []\n",
    "    for trace in log_fail:\n",
    "        alignments.append(align(trace, net, initial_marking, final_marking, model_cost_function, sync_cost_function))\n",
    "\n",
    "    pretty_print_alignments(alignments)\n",
    "\n",
    "\n",
    "#execute_script()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughput Time\n",
    "Para mirar la duración de cada caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_case_durations = pm4py.get_all_case_durations(log_fail)\n",
    "all_case_durations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cycle Time and Waiting Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.objects.log.util import interval_lifecycle\n",
    "enriched_log = interval_lifecycle.assign_lead_cycle_time(log_fail)\n",
    "enriched_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventualy follows graph are a useful tool too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efg_graph = pm4py.discover_eventually_follows_graph(log_fail)\n",
    "efg_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribución de la duración de un caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pm4py.util import constants\n",
    "from pm4py.statistics.traces.generic.log import case_statistics\n",
    "x, y = case_statistics.get_kde_caseduration(log_fail, parameters={constants.PARAMETER_CONSTANT_TIMESTAMP_KEY: \"time:timestamp\"})\n",
    "\n",
    "from pm4py.visualization.graphs import visualizer as graphs_visualizer\n",
    "\n",
    "gviz = graphs_visualizer.apply_plot(x, y, variant=graphs_visualizer.Variants.CASES)\n",
    "graphs_visualizer.view(gviz)\n",
    "\n",
    "gviz = graphs_visualizer.apply_semilogx(x, y, variant=graphs_visualizer.Variants.CASES)\n",
    "#graphs_visualizer.view(gviz)\n",
    "                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of events over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.algo.filtering.log.attributes import attributes_filter\n",
    "\n",
    "\n",
    "x, y = attributes_filter.get_kde_date_attribute(log_fail, attribute=\"time:timestamp\")\n",
    "\n",
    "from pm4py.visualization.graphs import visualizer as graphs_visualizer\n",
    "\n",
    "gviz = graphs_visualizer.apply_plot(x, y, variant=graphs_visualizer.Variants.DATES)\n",
    "graphs_visualizer.view(gviz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dotted Chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pm4py.view_dotted_chart(log_fail, format=\"png\")\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.algo.discovery.batches import algorithm\n",
    "batches = algorithm.apply(log_fail)\n",
    "batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Fitness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, marking, fmarking = pm4py.discover_petri_net_inductive(log_no_timeout)\n",
    "\n",
    "fitness = pm4py.fitness_alignments(log_fail, net, marking, fmarking)\n",
    "\n",
    "\n",
    "# Initialize an empty string to build the dictionary representation\n",
    "fitness_str = \"\"\n",
    "\n",
    "# Build the string with spaces and line breaks between each element\n",
    "for key, value in fitness.items():\n",
    "    fitness_str += f\"{key}: {value}\\n\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = pm4py.precision_alignments(log_fail, net, marking, fmarking)\n",
    "prec = \"precison: \"+ str(prec)\n",
    "prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.algo.evaluation.generalization import algorithm as generalization_evaluator\n",
    "\n",
    "gen = generalization_evaluator.apply(log_fail, net, marking, fmarking)\n",
    "gen=\"generalization: \" + str(gen)\n",
    "gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.algo.evaluation.simplicity import algorithm as simplicity_evaluator\n",
    "simp = simplicity_evaluator.apply(net)\n",
    "simp= \"simplicity: \" + str(simp)\n",
    "simp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the string to a text file, overwriting the previous content\n",
    "file_name = dir_tutti+\"/log/Tesis/mision_variables/model_evaluation.txt\"\n",
    "with open(file_name, \"w\") as file:\n",
    "    file.write(fitness_str)\n",
    "    file.write(prec+\"\\n\")\n",
    "    file.write(gen+\"\\n\")\n",
    "    file.write(simp+\"\\n\")\n",
    "\n",
    "\n",
    "copy_file(file_name, new_folder)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
